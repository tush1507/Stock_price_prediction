{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests\n",
    "# !pip install html5lib\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "def web_scrape(url_link,start,end):  \n",
    "    headlines = []\n",
    "    Published = []\n",
    "    pages = np.arange(start, end, 1)\n",
    " \n",
    "    for page in pages:\n",
    "        url = url_link + str(page)\n",
    "        r = requests.get(url,headers = {'User-Agent': 'Mozilla/5.0 '}).text\n",
    "        soup1 = BeautifulSoup(r)\n",
    "        # print('Page No:{}'.format(page))\n",
    "        count = 0\n",
    "        for link in soup1.find_all('a',{'class':'title'}, href=True, title=True):\n",
    "            for date in soup1.find_all(\"span\", class_=\"date\"):\n",
    "                sleep(randint(2, 10))\n",
    "                count = count+1\n",
    "                links = [link['title']]\n",
    "                dates = [date.text]\n",
    "                #print(links,dates) \n",
    "                for lines in links:\n",
    "                    if count > 6 and count <= 16:     #To avoid the noise/repetitive headlines                   \n",
    "                        headlines.append(lines)\n",
    "                for dt in dates:\n",
    "                    if count > 6 and count <= 16:      \n",
    "                        Published.append(dt[3:])\n",
    "                break\n",
    "    \n",
    "        df_start_end = pd.DataFrame({\"Headlines\":headlines,'Published_on':Published})\n",
    "#         return(df_start_end)    \n",
    "\n",
    "#These files will be stored in the specified folder when we will be executing the multithreading block\n",
    "    file = open(\"pushkar_output/page\"+\"_\"+str(start)+\"_to_\"+str(end), 'wb')\n",
    "\n",
    "    # dump information to that file\n",
    "    pickle.dump(df_start_end, file)\n",
    "\n",
    "    # close the file\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "threadlist = []\n",
    "#We provide batches for eg:here we see a batch of 20 pages\n",
    "#Depending on our system config we can tweak this\n",
    "for i in range(451,880,20):\n",
    "#     print(i,i+20)\n",
    "    t = threading.Thread(target=web_scrape , args = (\"https://www.investing.com/equities/google-inc-c-news/\",i,i+20,))\n",
    "    threadlist.append(t)\n",
    "    t.start()\n",
    "    \n",
    "for tr in threadlist:\n",
    "    tr.join()\n",
    "    print(\"done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to open our output pickle files.\n",
    "\n",
    "We see that each file conatins a list of Dataframes corresponding to respective batch.\n",
    "\n",
    "The last step is to merge output from all the files, manipulating the dataframe and exporting to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allcontacts = []\n",
    "\n",
    "for root, dirs, files in os.walk(\"pushkar_output/\"):\n",
    "    for f in files:\n",
    "#         print(f)\n",
    "#         f = files\n",
    "        opencontacts = open('C:/Users/pushk/Notebooks/pushkar_output/' + f,'rb')\n",
    "        loadedcontacts = pickle.load(opencontacts)\n",
    "#         print(loadedcontacts)\n",
    "        allcontacts.append(loadedcontacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "for i in range(len(allcontacts)):\n",
    "    df = final_df.append(allcontacts[i]).reset_index(drop = True)\n",
    "    final_df = df\n",
    "#     print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Published_on'] = pd.to_datetime(final_df['Published_on'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Published_on</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exclusive: U.S. agency probes Facebook for 'sy...</td>\n",
       "      <td>2021-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fueled by tech, Wall Street rebounds at end of...</td>\n",
       "      <td>2021-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S&amp;P 500 Snaps 2-Week Losing Streak as Dip Buyi...</td>\n",
       "      <td>2021-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biden adds Big Tech critic Tim Wu to his econo...</td>\n",
       "      <td>2021-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hackers target Williams F1 livery launch on AR...</td>\n",
       "      <td>2021-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8795</th>\n",
       "      <td>Indian firms could get boost from Google, Alibaba</td>\n",
       "      <td>2015-03-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8796</th>\n",
       "      <td>Rocket Internet-backed Helpling expands in Asi...</td>\n",
       "      <td>2015-03-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8797</th>\n",
       "      <td>DreamWorks, AwesomenessTV to make original sho...</td>\n",
       "      <td>2015-03-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8798</th>\n",
       "      <td>Wall St. rallies; Nasdaq hits highest level si...</td>\n",
       "      <td>2015-03-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8799</th>\n",
       "      <td>Why Are Amazon Shares Surging?</td>\n",
       "      <td>2015-03-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8800 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Headlines Published_on\n",
       "0     Exclusive: U.S. agency probes Facebook for 'sy...   2021-03-06\n",
       "1     Fueled by tech, Wall Street rebounds at end of...   2021-03-06\n",
       "2     S&P 500 Snaps 2-Week Losing Streak as Dip Buyi...   2021-03-06\n",
       "3     Biden adds Big Tech critic Tim Wu to his econo...   2021-03-06\n",
       "4     Hackers target Williams F1 livery launch on AR...   2021-03-06\n",
       "...                                                 ...          ...\n",
       "8795  Indian firms could get boost from Google, Alibaba   2015-03-12\n",
       "8796  Rocket Internet-backed Helpling expands in Asi...   2015-03-12\n",
       "8797  DreamWorks, AwesomenessTV to make original sho...   2015-03-12\n",
       "8798  Wall St. rallies; Nasdaq hits highest level si...   2015-03-12\n",
       "8799                    Why Are Amazon Shares Surging?    2015-03-12\n",
       "\n",
       "[8800 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = final_df.sort_values(by=['Published_on'],ascending=False)\n",
    "final_df.reset_index(inplace=True,drop=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('final_goog_headlines1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
